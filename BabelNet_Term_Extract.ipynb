{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for extracting BabelNet terms\n",
    "\n",
    "This Python code deals with the extraction of all the terms present in the Wordnet Knowledge Base, as well as cleaning the terms to remove the unnecessary articles such that it helps us to do classification in a better way."
    "\n"
    "To run the python script file on stout, command lines are python BabelNet_term_Extract.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing suitable libraries\n",
    "from itertools import islice\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Terms from BabelNet\n",
    " In the following code snipper we remove the determiners that appear at the beginning of the terms, such as 'a,an,the' etc. <br>Moreover, we also remove any presence of digits in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if any number appears in our string\n",
    "def hasNumbers(inputString):\n",
    "     return any(char.isdigit() for char in inputString)\n",
    "\n",
    "\n",
    "def removearticles(text):\n",
    "    #text is the raw term extracted from Wordnet\n",
    "    doc = nlp(text)\n",
    "    temp = []\n",
    "    i = 0\n",
    "    #Seperating determiners that appear at the beginning of the terms\n",
    "    while i<len(doc) and doc[i].pos_ == 'DET':\n",
    "        i+=1\n",
    "    #Appending the rest of the tokens of the term\n",
    "    while i!= len(doc):\n",
    "        temp.append(str(doc[i]))\n",
    "        i+=1\n",
    "\n",
    "    \n",
    "    fin = ' '.join(temp)\n",
    "    fin = re.sub(' ([@.#$\\/:-]) ?',r'\\1', fin)\n",
    "    return fin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating list and dictionary of cleaned terms\n",
    "The following code cleans the terms from BabelNet but also tries to not lose the data as present in BabelNet. A dictionary stores the cleaned term as the key, and it's value as the original lemma as found on BabelNet, and all the BabelSynsets which point to the same lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    list_of_terms = []\n",
    "    dictOfSynsets =  {}\n",
    "    #All the terms extracted from BabelNet via a Java code are present in a text file named \"senses_d.txt\"\n",
    "    with open('senses_d.txt', 'r') as sample:\n",
    "        \n",
    "        #Reading one terms at a time\n",
    "        lines = sample.readlines()\n",
    "       \n",
    "        \n",
    "        for word in lines:\n",
    "            word = word.lower()\n",
    "            data = word.split(\"#\")\n",
    "            #Uncleaned term\n",
    "            unclean = data[0]\n",
    "            #Replacing _ with spaces\n",
    "            data[0]=data[0].replace(\"_\",\" \")\n",
    "            data[0]=removearticles(data[0])\n",
    "            if data[0]!= [] and hasNumbers(data[0]) == False and len(word)!=0:\n",
    "\n",
    "                temp=[unclean]\n",
    "                #data[1] is the synset ID for the term\n",
    "                temp.append(data[1])\n",
    "                #Appening the synset ID if the term has appeared before\n",
    "                if data[0] in dictOfSynsets:\n",
    "                    dictOfSynsets[data[0]].append(data[1])\n",
    "                else:\n",
    "                    dictOfSynsets[data[0]] = temp\n",
    "            \n",
    "            \n",
    "\n",
    "    #The cleaned part from the BabelNet stored in a list\n",
    "    list_of_terms=list(dictOfSynsets.keys())\n",
    "    #Removing stopwords and punctuation marks in our extracted cleaned terms \n",
    "    nltk.download('stopwords')\n",
    "    list_of_terms = [word for word in list_of_terms if word not in stopwords.words('english')]\n",
    "    list_of_terms = [''.join(c for c in s if c not in string.punctuation) for s in list_of_terms]\n",
    "    list_of_terms = [s for s in list_of_terms if s]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling\n",
    "Since this list is huge and occupies a lot of memory, and also since we will need it later on, we store it in a compressed file called as pickle. The process of generating this list everytime we need to pre-process is expensive and hence pickling is essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Pickling our List of cleaned terms\n",
    "with open('outfile2', 'wb') as fp:\n",
    "    pickle.dump(list_of_terms, fp)\n",
    "#Pickling the dictionary    \n",
    "with open('outfile_dict', 'wb') as fpp:\n",
    "    pickle.dump(dictOfSynsets, fpp)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
