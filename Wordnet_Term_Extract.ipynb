{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for extracting Wordnet terms\n",
    "\n",
    "This Python code deals with the extraction of all the terms present in the Wordnet Knowledge Base, as well as cleaning the terms to remove the unnecessary articles such that it helps us to do classification in a better way.<br>\n",
    "To run the python script file on stout, command lines are **python WordNet_term_Extract.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing suitable libraries\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import islice\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Terms from WordNet\n",
    " In the following code snipper we remove the determiners that appear at the beginning of the terms, such as 'a,an,the' etc. <br>Moreover, we also remove any presence of digits in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removearticles(text):\n",
    "    #text is the raw term extracted from Wordnet\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    temp = []\n",
    "    i = 0\n",
    "    #Seperating determiners that appear at the beginning of the terms\n",
    "    while i<len(doc) and doc[i].pos_ == 'DET':\n",
    "        \n",
    "        i+=1\n",
    "    #Appending the rest of the tokens of the term\n",
    "    while i!= len(doc):\n",
    "        temp.append(str(doc[i]))\n",
    "        i+=1\n",
    "\n",
    "    \n",
    "    fin = ' '.join(temp)\n",
    "    fin = re.sub(' ([@.#$\\/:-]) ?',r'\\1', fin)\n",
    "    return fin\n",
    "\n",
    "    #Checking if any number appears in our string\n",
    "def hasNumbers(inputString):\n",
    "     return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words consists of all terms found on WordNet\n",
    "all_words = []\n",
    "for word in wn.words():\n",
    "    all_words.append(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating list and dictionary of cleaned terms\n",
    "The following code cleans the terms from WordNet but also tries to not lose the data as present in WordNet. A dictionary stores the cleaned term as the key, and it's value as the original way the term was found on WordNet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_terms = []\n",
    "dictOfLemma =  {}\n",
    "for w in all_words:\n",
    "    lemmaword = w\n",
    "    if hasNumbers(w) == True:\n",
    "        continue\n",
    "    w=w.replace(\"_\",\" \")\n",
    "    w = removearticles(w)\n",
    "    dictOfLemma[w] = lemmaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cleaned part from the WordNet stored in a list\n",
    "list_of_terms=list(dictOfLemma.keys())\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "#Removing stopwords and punctuation marks in our extracted cleaned terms \n",
    "list_of_terms = [word for word in list_of_terms if word not in stopwords.words('english')]\n",
    "list_of_terms = [''.join(c for c in s if c not in string.punctuation) for s in list_of_terms]\n",
    "list_of_terms = [s for s in list_of_terms if s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling\n",
    "Since this list is huge and occupies a lot of memory, and also since we will need it later on, we store it in a compressed file called as pickle. The process of generating this list everytime we need to pre-process is expensive and hence pickling is essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "#Pickling our List of cleaned terms\n",
    "with open('list_wordnet', 'wb') as fp:\n",
    "    pickle.dump(list_of_terms, fp)\n",
    "#Pickling the dictionary\n",
    "with open('dict_wordnet', 'wb') as fpp:\n",
    "    pickle.dump(dictOfLemma, fpp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
