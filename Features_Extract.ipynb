{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and Extracting Features\n",
    "\n",
    "This python code is responsible for carrying out the entore preprocessing and feature extraction from the corpus, and also creating the feature vector from the terms that are annotatted on the sentence, using the external knowledge base. It also validates from the distant resource whether a pair of terms is a valid example of a hypernym relation or not. <br>\n",
    "To run the python script file on stout, command lines are **python Features_Extract.py >> output.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Python Modules\n",
    "We first import all the modules that will be required for the entire process, at once. These include NLP libraries such as spaCy, and NLTK.<br>                                                            We also load the English functions from these modules since our analysis will be based on an English corpus. <br>\n",
    "<br>\n",
    "Pymagnitude is a python module which helps us to load embeddings of grammatical and syntactic features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import subprocess\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import wordnet as wn\n",
    "nlp = en_core_web_sm.load()\n",
    "import pymagnitude\n",
    "import numpy as np\n",
    "from pymagnitude import *\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating Dictionary\n",
    "Since we do our processing with our input as a sentence, the function below will create a dictionary with keys as the tokens of the sentence.<br> The values to these keys will be some particular features which can be directly extracted with the help of spaCy's inbuilt functions.<br> Once we build this dictionary, we can pass this to the function that will eventually create our feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_dict(test_text,terms):\n",
    "    listofTokens = []\n",
    "    listofFeatures = []\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #Converting each and every word of the sentence to lower case\n",
    "    test_text = test_text.lower()\n",
    "    #Tokenizing our sentence\n",
    "    word_list = nltk.word_tokenize(test_text)\n",
    "    #Lemmatizing each and every token\n",
    "    test_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    doc = nlp(test_text)\n",
    "    #print(doc)\n",
    "    for i in range(len(doc)):\n",
    "         # print(\"Token text:\",doc[i].text, \"Token Lemma:\", doc[i].lemma_,\"Token POS :\", doc[i].pos_,\"Token tag:\", doc[i].tag_, \"Token dependency:\",doc[i].dep_,\n",
    "         #       \"Token Shape:\", doc[i].shape_,\"Is token alphabetic:\", doc[i].is_alpha,\"Is it a stop word: \", doc[i].is_stop)\n",
    "         #Storing each tokens of the sentence in a list\n",
    "         listofTokens.append(str(doc[i]))\n",
    "         temp_list = [doc[i].lemma_,doc[i].pos_,doc[i].tag_,doc[i].dep_,doc[i].shape_,doc[i].is_alpha,doc[i].is_stop]\n",
    "         #We store each Lemma, POS, Part-of-speech tag, dependency tag,shape of the token,if it is alphanumeric, and if it is a stopword\n",
    "         listofFeatures.append(temp_list)\n",
    "    #print(listofTokens)\n",
    "\n",
    "    #print(listofFeatures)\n",
    "    zipbObj = zip(listofTokens, listofFeatures)\n",
    "    #Creating a dictionary with key as the token, and its value as its certain features\n",
    "    dictOfWords = dict(zipbObj)\n",
    "    #Named-Entity-Recognition for tokens which are present in the sentence\n",
    "    for ent in doc.ents:\n",
    "        if(ent.text in listofTokens):\n",
    "            dictOfWords[ent.text].append(ent.label_)\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    print(dictOfWords)\n",
    "    #Calling the features_extract function to extract features and create the feature vector\n",
    "    features_extract(test_text,dictOfWords,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Feature Extraction Function\n",
    "In this function we have as arguments our sentence (test_text), the dictionary with keys as tokens of this sentence and values as their grammatical and syntactic features, as well as the list of terms  from the external Knowledge Base. <br> Moreover it implements our approach of annotating terms found on the sentence using spaCy's phraseMatcher. It also describes a method that would retrieve features of at max 3 terms, left of the first term, and right of the second term.<br>The function takes into consideration the window of maximum 10 tokens between the pair of terms. In addition, it also retrieves features from the token sequence that appear on the shortest dependency path between the two terms.<br><br>\n",
    "       We used Pymagnitude module to encode our syntactic and grammatical features into 4-dimensional embeddings, to integrate that in our feature vector. These embeddings are used repeatedly for every token we examine to be part of our feature vector.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def features_extract(test_text,dictOfWords,terms):\n",
    "    \n",
    "    #Creating a copy of terms from external KB\n",
    "    terms_temp = terms.copy()\n",
    "    #Using pymagnitude module to retrieve embeddings of POS and dependency tags\n",
    "    pos_vectors = FeaturizerMagnitude(100, namespace = \"PartsOfSpeech\")\n",
    "    dependency_vectors = FeaturizerMagnitude(100, namespace = \"SyntaxDependencies\")\n",
    "    pos_vectors.dim\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    #Loading PhraseMatcher from spaCy\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #dictionaries for storing features of tokens before and after the terms\n",
    "    dictbefore={}\n",
    "    dictafter={}\n",
    "\n",
    "\n",
    "    #Stripping punctuation marks from the sentence\n",
    "    test_text = test_text.strip('\"')\n",
    "    print(test_text)\n",
    "    \n",
    "    \n",
    "    # Only run nlp.make_doc to speed things up\n",
    "    patterns = [nlp.make_doc(text) for text in terms]\n",
    "    matcher.add(\"TerminologyList\", None, *patterns)\n",
    "    #Making our sentence a spaCy object\n",
    "    doc = nlp(test_text)\n",
    "    #matches is a list that will store the start index, end index and the match id of the term\n",
    "    matches = matcher(doc)\n",
    "    start_idx = []\n",
    "    end_idx = []\n",
    "    # Flagit is a list which will avoid duplication of terms in the sentence\n",
    "    flagit = []\n",
    "    new_matches = []\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        #the term matched\n",
    "        span = doc[start:end]\n",
    "        \n",
    "        if span in flagit:\n",
    "            continue\n",
    "        #If the term has more than one token\n",
    "        if end - start>1:\n",
    "            #the following code snippet ensures that only the term with the most tokens will be selected\n",
    "            for item in matches:\n",
    "            \n",
    "                if item[1] >=start and item[1]<end:\n",
    "                    \n",
    "                    \n",
    "                    flag_term = doc[item[1]:item[2]]\n",
    "                   \n",
    "                    if item in new_matches:\n",
    "                       \n",
    "                        new_matches.remove(item)\n",
    "                    flagit.append(flag_term)\n",
    "            \n",
    "            for i in range(start,end,1):\n",
    "                \n",
    "                #After selecting the largest length for the term, we remove the tokens that constitute it from the term list\n",
    "                t1 = str(doc[i:i+1])\n",
    "                \n",
    "                if t1 in terms_temp:\n",
    "                    \n",
    "                    if i in start_idx:\n",
    "                        start_idx.remove(i)\n",
    "                    if i in end_idx:\n",
    "                        end_idx.remove(i)\n",
    "                    try:\n",
    "                        terms.remove(t1)\n",
    "                    except:\n",
    "                        continue\n",
    "       \n",
    "        #new matches will discard all the duplicate terms and terms which were part of a larger term\n",
    "        add_match_entity = [match_id,start,end]\n",
    "        new_matches.append(add_match_entity)\n",
    "        start_idx.append(start)\n",
    "        end_idx.append(end-1)\n",
    "        print(\"term matched:\",span.text)\n",
    "      \n",
    "    size = len(doc)\n",
    "    cnt=0\n",
    "    print(start_idx)\n",
    "    print(end_idx)\n",
    "    for i in range(len(doc)):\n",
    "    \n",
    "        #We look at feautres of tokens before the first term\n",
    "        if i in start_idx :\n",
    "            id = start_idx.index(i)\n",
    "            \n",
    "            cnt+=1\n",
    "            print(\"Tokens before T\"+str(cnt))\n",
    "            print(i)\n",
    "        \n",
    "            temp = []\n",
    "        #Since we fixed the max terms to look only at 3 terms, this is for checking the corner cases\n",
    "            if i < 3:\n",
    "                for j in range(i-1,-1,-1) :\n",
    "                    if j in end_idx:\n",
    "                       break\n",
    "                    print(doc[j],\":\",dictOfWords[str(doc[j])])\n",
    "                    temp.append(dictOfWords[str(doc[j])])\n",
    "            else:\n",
    "                for j in range(i-1,i-4,-1):\n",
    "                    if j in end_idx:\n",
    "                        break\n",
    "                    print(doc[j],\":\",dictOfWords[str(doc[j])])\n",
    "                    temp.append(dictOfWords[str(doc[j])])\n",
    "            dictbefore[\"T\"+str(cnt)] = temp\n",
    "       #We look at features after the second term\n",
    "            k = end_idx[id]\n",
    "            temp = []\n",
    "            print(\"Tokens after T\"+str(cnt))\n",
    "        #Corner case since we check for at max 3 terms after the second term\n",
    "            if i > len(doc) - 4:\n",
    "                for j in range(k+1,size-1,1):\n",
    "                    if j in start_idx:\n",
    "                        break\n",
    "                    print(doc[j],\":\",dictOfWords[str(doc[j])])\n",
    "                    temp.append(dictOfWords[str(doc[j])])\n",
    "            else:\n",
    "                for j in range(k+1,k+4,1):\n",
    "                    if j in start_idx:\n",
    "                        break\n",
    "                    print(doc[j],\":\",dictOfWords[str(doc[j])])\n",
    "                    temp.append(dictOfWords[str(doc[j])])\n",
    "            dictafter[\"T\"+str(cnt)] = temp\n",
    "\n",
    "                \n",
    "    for key in dictbefore:\n",
    "        #Default value for our embeddings is set as [1,1,1,1]\n",
    "        pos_temp = np.float64([1,1,1,1])        \n",
    "        tag_temp = np.float64([1,1,1,1])\n",
    "        dependency_temp = np.float64([1,1,1,1])\n",
    "    \n",
    "    \n",
    "        #To take into account the sequence in which the tokens appear, we multiply the embeddings of their features such that it will be unique for each sequence\n",
    "        for i in range(0,len(dictbefore[key]),1):\n",
    "                        pos_temp *= np.array(pos_vectors.query(dictbefore[key][i][1]))\n",
    "                        tag_temp *= np.array(pos_vectors.query(dictbefore[key][i][2]))\n",
    "                        dependency_temp *= np.array(dependency_vectors.query(dictbefore[key][i][3]))\n",
    "                \n",
    "        #Converting numpy array to a list and then merging all the lists into one\n",
    "        \n",
    "        #Needs to be converted to list for babelnet \n",
    "        #pos_temp = pos_temp.tolist()\n",
    "        #tag_temp = tag_temp.tolist()\n",
    "        #dependency_temp = dependency_temp.tolist()\n",
    "        temp = [pos_temp,tag_temp,dependency_temp]\n",
    "        temp = [val for sublist in temp for val in sublist]\n",
    "        dictbefore[key] = temp\n",
    "   \n",
    "    \n",
    "    #Doing the same as above for the terms after the second term\n",
    "    for key in dictafter:\n",
    "        pos_temp = np.float64([1,1,1,1])        \n",
    "        tag_temp = np.float64([1,1,1,1])\n",
    "        dependency_temp = np.float64([1,1,1,1])\n",
    "   \n",
    "        for i in range(0,len(dictafter[key]),1):\n",
    "                \n",
    "                    pos_temp *= np.array(pos_vectors.query(dictafter[key][i][1]))\n",
    "                    tag_temp *= np.array(pos_vectors.query(dictafter[key][i][2]))\n",
    "                    dependency_temp *= np.array(dependency_vectors.query(dictafter[key][i][3]))\n",
    "                \n",
    "        #Needs to be converted to list for babelnet  \n",
    "        #pos_temp = pos_temp.tolist()\n",
    "        #tag_temp = tag_temp.tolist()\n",
    "        #dependency_temp = dependency_temp.tolist()\n",
    "        temp = [pos_temp,tag_temp,dependency_temp]\n",
    "        temp = [val for sublist in temp for val in sublist]\n",
    "        dictafter[key] = temp\n",
    "\n",
    "    dictentity = {}\n",
    "    count=1;\n",
    "\n",
    "    #The following code snippet is for the shortest dependency path\n",
    "    for match_id, start, end in new_matches:\n",
    "        span = doc[start:end]\n",
    "       \n",
    "        if start not in start_idx or end-1 not in end_idx:\n",
    "            continue\n",
    "        \n",
    "      \n",
    "        flag=1\n",
    "        if end - start > 1:\n",
    "            for i in range(start,end,1):\n",
    "                temp = dictOfWords[str(doc[i])]\n",
    "        \n",
    "            #Since shortest dependency path is token based,\n",
    "            #for terms with more than one tokens, we will choose only\n",
    "            #the token that is a noun or the rightmost token\n",
    "        \n",
    "                if temp[1] in {\"PROPN\",\"NOUN\"}:\n",
    "                    flag=0\n",
    "                    dictentity[\"T\"+str(count)] = str(doc[i])\n",
    "            if flag==1:\n",
    "                    dictentity[\"T\"+str(count)] = str(doc[end-1])\n",
    "        else:\n",
    "            dictentity[\"T\"+str(count)] = str(span)\n",
    "        count = count + 1\n",
    "        import networkx as nx\n",
    "    dictSDP={}\n",
    "    #Assigning shortest dependency from term1 to term2\n",
    "    for token in doc:\n",
    "        print((token.head.text, token.text, token.dep_))\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token.lower_),\n",
    "                          '{0}'.format(child.lower_)))\n",
    "            \n",
    "    \n",
    "    #Constructing features for the tokens that are present in the shortest dependency path the same way we did earlier\n",
    "    for i in range(0,len(dictentity),1):\n",
    "        for j in range(i+1,len(dictentity),1):\n",
    "            entity1 = dictentity[\"T\"+str(i+1)]\n",
    "            entity2 = dictentity[\"T\"+str(j+1)]\n",
    "            vec_temp = np.float64([1,1,1,1])\n",
    "            dictSDP[str(i+1)+str(j+1)]  = vec_temp\n",
    "            \n",
    "            #If both refer to the same term, then continue\n",
    "            if entity1 == entity2:                \n",
    "                continue\n",
    "            graph = nx.Graph(edges)\n",
    "            #If there exists a shortest dependency path between the two tokens, then we build the features with embeddings\n",
    "            try:\n",
    "                print(\"Shortest path length betwen T\"+str(i+1),\"and T\"+str(j+1),\":\",nx.shortest_path_length(graph, source=entity1, target=entity2))\n",
    "                path=nx.shortest_path(graph, source=entity1, target=entity2)\n",
    "                print(\"Tokens in shortest path:\",path)\n",
    "                vec = []\n",
    "                dep_temp = np.float64([1,1,1,1])\n",
    "                print(\"Dependency tags of the tokens:\")\n",
    "                for x in path:\n",
    "                    print(dictOfWords[x][3])\n",
    "                    dep_temp *= np.array(dependency_vectors.query(dictOfWords[x][3]))\n",
    "                    dep_temp = dep_temp.tolist()\n",
    "                    vec = dep_temp\n",
    "                    dictSDP[str(i+1)+str(j+1)]  = vec    \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            features_list = []\n",
    "            \n",
    "    #The following for loops are solely responsible for creating the feature vectors\n",
    "    for i in range(len(start_idx)):\n",
    "        for j in range(i+1,len(end_idx),1):\n",
    "            \n",
    "            #This takes care of the fact that the window for the two terms should be at max 10 tokens\n",
    "            if start_idx[j] - end_idx[i] <= 10 and end_idx[j]!=end_idx[i] and start_idx[j]!=start_idx[i]: \n",
    "                termA = \"T\"+str(i+1)\n",
    "                termB = \"T\"+str(j+1)\n",
    "                \n",
    "                \n",
    "                if dictentity[termA] == dictentity[termB]:\n",
    "                    continue\n",
    "                print(\"Distance between T\"+str(i+1)+\" and T\"+str(j+1)+\":\", start_idx[j]- end_idx[i] -1)\n",
    "                print(\"Tokens between T\"+str(i+1)+\" and T\"+str(j+1))\n",
    "                print(\"the terms:\" ,dictentity[\"T\"+str(i+1)], dictentity[\"T\"+str(j+1)])\n",
    "                file='hyp2.txt' \n",
    "                t1 = str(dictentity[\"T\"+str(i+1)])\n",
    "                t2 = str(dictentity[\"T\"+str(j+1)])\n",
    "                \n",
    "                \n",
    "                #Checker is 0 if it is not hypernym relation and 1 if it is.\n",
    "                checker = 0\n",
    "                #All synsets for the lemmas of term1 and term2 from WordNet\n",
    "                size1 = len(wn.synsets(t1))\n",
    "                size2 = len(wn.synsets(t2))\n",
    "                \n",
    "                \n",
    "                #To check if term2 is hypernym of term1 \n",
    "                for i1 in range(0,size1,1):\n",
    "                    if checker == 1:\n",
    "                        break\n",
    "                    t1_syn = wn.synsets(t1)[i1]\n",
    "                    for j1 in range(0,size2,1):\n",
    "                        t2_syn = wn.synsets(t2)[j1]\n",
    "                        hypo1 = set([i1 for i1 in t1_syn.closure(lambda s:s.hyponyms())])\n",
    "                      \n",
    "                        if t2_syn in hypo1 :\n",
    "                            checker = 1\n",
    "                            \n",
    "               #To check if term1 is hypernym of term2         \n",
    "                \n",
    "                for i1 in range(0,size2,1):\n",
    "                    if checker == 1:\n",
    "                        break\n",
    "                    t2_syn = wn.synsets(t2)[i1]\n",
    "                    for j1 in range(0,size1,1):\n",
    "                        t1_syn = wn.synsets(t1)[j1]\n",
    "                        hypo2 = set([i1 for i1 in t2_syn.closure(lambda s:s.hyponyms())])\n",
    "                       # print(hypo2)\n",
    "                        if t2_syn in hypo2 :\n",
    "                            checker = 1\n",
    "\n",
    "                print(checker)\n",
    "                \n",
    "                \n",
    "                #This part is for Babelnet\n",
    "               # with open(file, 'w') as filetowrite:\n",
    "               #         t1 = str(dictentity[\"T\"+str(i+1)])\n",
    "               #         t2 = str(dictentity[\"T\"+str(j+1)])\n",
    "               #         filetowrite.write(dict_of_terms[t1][0]+\";\"+dict_of_terms[t2][0])\n",
    "                \n",
    "               # out = subprocess.Popen(['bash','hyp_check2.sh'], \n",
    "               #     stdout=subprocess.PIPE, \n",
    "               #     stderr=subprocess.STDOUT)\n",
    "               # stdout,stderr = out.communicate()\n",
    "               # x=stdout.splitlines()\n",
    "               # result= str(x[-1])\n",
    "               # result = result.split(\";\")\n",
    "               # tf = result[-1]\n",
    "               # tf = tf[:-1]\n",
    "               \n",
    "               # pos_temp = np.float64([1,1,1,1])\n",
    "                \n",
    "                \n",
    "               #Building features from tokens between the terms same way as done for tokens before and after \n",
    "                tag_temp = np.float64([1,1,1,1])\n",
    "                dependency_temp = np.float64([1,1,1,1])\n",
    "                for k in range(end_idx[i]+1,start_idx[j],1):\n",
    "                    \n",
    "                    temp = []\n",
    "                    pos_temp *= np.array(pos_vectors.query(dictOfWords[str(doc[k])][1]))\n",
    "                    tag_temp *= np.array(pos_vectors.query(dictOfWords[str(doc[k])][2]))\n",
    "                    dependency_temp *= np.array(dependency_vectors.query(dictOfWords[str(doc[k])][3]))\n",
    "\n",
    "               #Needs to be converted to list for babelnet   \n",
    "               # pos_temp = pos_temp.tolist()\n",
    "               # tag_temp = tag_temp.tolist()\n",
    "               # dependency_temp = dependency_temp.tolist()\n",
    "             \n",
    "                #Combining all the feature list\n",
    "                try:\n",
    "                    temp = [dictbefore[termA],pos_temp,tag_temp,dependency_temp,dictafter[termB],dictSDP[str(i+1)+str(j+1)]]\n",
    "                    \n",
    "                \n",
    "                except:\n",
    "                    continue\n",
    "                #Merging all the feature lists to a feature vector\n",
    "                flattened = [val for sublist in temp for val in sublist]\n",
    "                flattened.insert(0,start_idx[j]- end_idx[i] -1)\n",
    "                \n",
    "                \n",
    "                flattened.append(checker)\n",
    "                        \n",
    "                print(flattened)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is responsible to retrieve the list of terms from the Knowledge Base as well as the dictionary that corresponds to its original form or synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('list_wordnet', 'rb') as fp:\n",
    "    list_of_terms = pickle.load(fp)\n",
    "with open ('dict_wordnet', 'rb') as fp:\n",
    "    dict_of_terms = pickle.load(fp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multiprocessing \n",
    "  As the entire preprocessing task is very taxing and time consuming for the CPU , we try to divide the tasks parallely among the number of CPUs present on the Stout cluster. This improves the overall performance of our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running it on the Corpus\n",
    "   The following code tries to generate sample positive and negative examples from 10 pages of the corpus, built for the process of testing our preprocessing and feature extracting technique. We also call implement pooling to improve the time consumption ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "#To run over 10 pages of our corpus\n",
    "for i in range(0,9):\n",
    "    #Creating base string to help in automation\n",
    "    path = '/data/bphukan/webbase_all/delorme.com_shu.pages_'\n",
    "    path+=str(i)+'.txt'\n",
    "    with open(path, 'r') as f:\n",
    "        nltk.download('punkt')\n",
    "        #Reading one line at a time from the text file\n",
    "        line = f.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "           #a_list contains the list of sentences from the line currently being read\n",
    "           a_list = nltk.tokenize.sent_tokenize(line)\n",
    "            #For each sentence in a_list\n",
    "           for line_text in a_list:\n",
    "                #To check if the sentence is NULL or only has whitespaces\n",
    "               if line_text and line_text.isspace()== False:\n",
    "                    #Removing all the punctuation marks in the sentence\n",
    "                    line_text=line_text.strip(string.punctuation)\n",
    "                    \n",
    "                    #Applying the CPU pooling to our functions for preprocessing\n",
    "                    results = [pool.apply(create_dict, args=(line_text,list_of_terms))]\n",
    "                    print(results)\n",
    "\n",
    "                    \n",
    "                   \n",
    "           line = f.readline()\n",
    "pool.close()\n",
    "       \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
